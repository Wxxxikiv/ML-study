{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70c4e913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# 数据集类定义\n",
    "class CatAndDogDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for file_name in self.data_dir.iterdir():\n",
    "            if file_name.is_file() and file_name.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
    "                self.image_paths.append(file_name)\n",
    "                if 'cat' in file_name.stem.lower():\n",
    "                    self.labels.append(0)\n",
    "                elif 'dog' in file_name.stem.lower():\n",
    "                    self.labels.append(1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# 定义训练和验证的转换\n",
    "def get_transforms():\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(\n",
    "            brightness=0.2, \n",
    "            contrast=0.2, \n",
    "            saturation=0.2, \n",
    "            hue=0.1\n",
    "        ),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d796f5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_mean_std_fast(data_dir, sample_ratio=0.1, batch_size=128, num_workers=0):\n",
    "#     \"\"\"\n",
    "#     使用采样法快速计算数据集的均值和标准差\n",
    "#     适用于大规模数据集（如12500个文件）\n",
    "    \n",
    "#     参数:\n",
    "#         data_dir: 数据集目录路径\n",
    "#         sample_ratio: 采样比例（0.1表示10%）\n",
    "#         batch_size: 批次大小\n",
    "#         num_workers: 数据加载的工作进程数（在Jupyter环境中建议设为0）\n",
    "    \n",
    "#     返回:\n",
    "#         mean: 均值数组 [R, G, B]\n",
    "#         std: 标准差数组 [R, G, B]\n",
    "#     \"\"\"\n",
    "#     # 定义基础转换（仅调整大小和转张量）\n",
    "#     basic_transform = transforms.Compose([\n",
    "#         transforms.Resize((224, 224)),  # 统一大小\n",
    "#         transforms.ToTensor(),          # 转张量并归一化到[0,1]\n",
    "#     ])\n",
    "    \n",
    "#     # 创建完整数据集\n",
    "#     full_dataset = CatAndDogDataset(\n",
    "#         data_dir=data_dir,\n",
    "#         transform=basic_transform,\n",
    "#         has_label=True\n",
    "#     )\n",
    "    \n",
    "#     # 采样部分数据\n",
    "#     dataset_size = len(full_dataset)\n",
    "#     sample_size = max(1, int(dataset_size * sample_ratio))  # 确保至少有1个样本\n",
    "#     sample_indices = random.sample(range(dataset_size), sample_size)\n",
    "#     sampled_dataset = Subset(full_dataset, sample_indices)\n",
    "    \n",
    "#     # 创建数据加载器（禁用多进程以避免序列化问题）\n",
    "#     dataloader = DataLoader(\n",
    "#         sampled_dataset,\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=False,\n",
    "#         num_workers=num_workers,  # 在Jupyter环境中设置为0\n",
    "#         pin_memory=False,         # 单进程模式下不需要\n",
    "#         drop_last=False\n",
    "#     )\n",
    "    \n",
    "#     # 使用CPU计算（简化实现）\n",
    "#     device = torch.device(\"cpu\")\n",
    "#     print(f\"使用设备: {device}\")\n",
    "#     print(f\"数据集大小: {dataset_size}\")\n",
    "#     print(f\"采样数量: {sample_size}\")\n",
    "    \n",
    "#     # 初始化均值计算\n",
    "#     mean = torch.zeros(3, device=device)\n",
    "#     total_pixels = 0\n",
    "    \n",
    "#     print(f\"计算均值中 (使用{sample_size}个样本)...\")\n",
    "#     for images, _ in tqdm(dataloader):\n",
    "#         images = images.to(device)\n",
    "#         batch_samples = images.size(0)\n",
    "        \n",
    "#         # 计算当前批次的均值\n",
    "#         batch_mean = torch.mean(images, dim=[0, 2, 3])\n",
    "#         mean += batch_mean * batch_samples\n",
    "#         total_pixels += batch_samples\n",
    "    \n",
    "#     # 计算最终均值\n",
    "#     mean /= total_pixels\n",
    "    \n",
    "#     # 初始化方差计算\n",
    "#     var = torch.zeros(3, device=device)\n",
    "    \n",
    "#     print(\"计算标准差中...\")\n",
    "#     for images, _ in tqdm(dataloader):\n",
    "#         images = images.to(device)\n",
    "#         batch_samples = images.size(0)\n",
    "        \n",
    "#         # 减去均值\n",
    "#         for i in range(3):\n",
    "#             images[:, i, :, :] -= mean[i]\n",
    "        \n",
    "#         # 计算方差\n",
    "#         batch_var = torch.mean(images ** 2, dim=[0, 2, 3])\n",
    "#         var += batch_var * batch_samples\n",
    "    \n",
    "#     # 计算最终标准差\n",
    "#     std = torch.sqrt(var / total_pixels)\n",
    "    \n",
    "#     # 转回CPU并转为列表\n",
    "#     mean = mean.cpu().tolist()\n",
    "#     std = std.cpu().tolist()\n",
    "    \n",
    "#     return mean, std\n",
    "\n",
    "# # 简单的单进程计算方法（适合调试）\n",
    "# def calculate_mean_std_simple(data_dir, sample_ratio=0.1):\n",
    "#     \"\"\"\n",
    "#     简单的单进程方法计算均值和标准差，适合调试\n",
    "    \n",
    "#     参数:\n",
    "#         data_dir: 数据集目录路径\n",
    "#         sample_ratio: 采样比例（0.1表示10%）\n",
    "    \n",
    "#     返回:\n",
    "#         mean: 均值数组 [R, G, B]\n",
    "#         std: 标准差数组 [R, G, B]\n",
    "#     \"\"\"\n",
    "#     # 定义基础转换\n",
    "#     basic_transform = transforms.Compose([\n",
    "#         transforms.Resize((224, 224)),\n",
    "#         transforms.ToTensor(),\n",
    "#     ])\n",
    "    \n",
    "#     # 创建数据集\n",
    "#     dataset = CatAndDogDataset(\n",
    "#         data_dir=data_dir,\n",
    "#         transform=basic_transform,\n",
    "#         has_label=True\n",
    "#     )\n",
    "    \n",
    "#     # 采样数据\n",
    "#     dataset_size = len(dataset)\n",
    "#     sample_size = max(1, int(dataset_size * sample_ratio))\n",
    "#     sample_indices = random.sample(range(dataset_size), sample_size)\n",
    "    \n",
    "#     # 初始化均值和标准差计算\n",
    "#     mean = torch.zeros(3)\n",
    "#     std = torch.zeros(3)\n",
    "    \n",
    "#     print(f\"使用简单方法计算均值和标准差 (样本数: {sample_size})...\")\n",
    "    \n",
    "#     # 计算均值\n",
    "#     for idx in tqdm(sample_indices):\n",
    "#         image, _ = dataset[idx]\n",
    "#         mean += torch.mean(image, dim=[1, 2])\n",
    "#     mean /= sample_size\n",
    "    \n",
    "#     # 计算标准差\n",
    "#     for idx in tqdm(sample_indices):\n",
    "#         image, _ = dataset[idx]\n",
    "#         for i in range(3):\n",
    "#             image[i] -= mean[i]\n",
    "#         std += torch.mean(image ** 2, dim=[1, 2])\n",
    "#     std = torch.sqrt(std / sample_size)\n",
    "    \n",
    "#     # 转为列表\n",
    "#     mean = mean.tolist()\n",
    "#     std = std.tolist()\n",
    "    \n",
    "#     return mean, std\n",
    "\n",
    "# # 使用示例\n",
    "# if __name__ == \"__main__\":\n",
    "#     data_dir = '/Users/sunchangxing/Documents/project/ML-study/torch/kaggle/cat_and_dog/data/train'\n",
    "    \n",
    "#     try:\n",
    "#         # 方法1: 修复后的采样法（适合大数据集）\n",
    "#         print(\"=== 使用修复后的采样法计算均值和标准差 ===\")\n",
    "#         # 注意：设置num_workers=0避免多进程问题\n",
    "#         mean, std = calculate_mean_std_fast(data_dir, sample_ratio=0.15, batch_size=128, num_workers=0)\n",
    "#         print(f\"均值: {mean}\")\n",
    "#         print(f\"标准差: {std}\")\n",
    "#         print(f\"可直接用于transforms.Normalize: transforms.Normalize(mean={mean}, std={std})\")\n",
    "        \n",
    "#         # 方法2: 简单单进程方法（适合调试）\n",
    "#         # print(\"\\n=== 使用简单单进程方法计算均值和标准差 ===\")\n",
    "#         # mean, std = calculate_mean_std_simple(data_dir, sample_ratio=0.05)\n",
    "#         # print(f\"均值: {mean}\")\n",
    "#         # print(f\"标准差: {std}\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"发生错误: {e}\")\n",
    "#         print(\"请检查数据集路径是否正确，以及数据格式是否符合要求\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a04441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform, val_transform = get_transforms()\n",
    "\n",
    "dataset = CatAndDogDataset(\"data/train\", transform=train_transform)\n",
    "\n",
    "batch_size = 32\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(\n",
    "    dataset, \n",
    "    [train_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # 设置随机种子以确保结果可复现\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    pin_memory=True\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44b7b223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256*14*14, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 2),\n",
    "            # nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "412d958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(test_loader.dataset)\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61909c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.8222, Test Loss: 0.6308, Test Acc: 0.6414\n",
      "Epoch 2/10, Train Loss: 0.5851, Test Loss: 0.5449, Test Acc: 0.7160\n",
      "Epoch 3/10, Train Loss: 0.5217, Test Loss: 0.4978, Test Acc: 0.7250\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m epoch_num = \u001b[32m10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch_num):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     train_loss = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     test_loss, test_acc = test(model, test_loader, criterion, device)\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Test Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_loader, criterion, optimizer, device)\u001b[39m\n\u001b[32m      2\u001b[39m model.train()\n\u001b[32m      3\u001b[39m running_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/pytorch_study/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/pytorch_study/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/pytorch_study/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/pytorch_study/lib/python3.12/site-packages/torch/utils/data/dataset.py:420\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mCatAndDogDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     28\u001b[39m label = \u001b[38;5;28mself\u001b[39m.labels[idx]\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     image = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/pytorch_study/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/pytorch_study/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/pytorch_study/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/pytorch_study/lib/python3.12/site-packages/torchvision/transforms/transforms.py:1276\u001b[39m, in \u001b[36mColorJitter.forward\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m   1274\u001b[39m     img = F.adjust_brightness(img, brightness_factor)\n\u001b[32m   1275\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m fn_id == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m contrast_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1276\u001b[39m     img = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43madjust_contrast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrast_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m fn_id == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m saturation_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1278\u001b[39m     img = F.adjust_saturation(img, saturation_factor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/pytorch_study/lib/python3.12/site-packages/torchvision/transforms/functional.py:907\u001b[39m, in \u001b[36madjust_contrast\u001b[39m\u001b[34m(img, contrast_factor)\u001b[39m\n\u001b[32m    905\u001b[39m     _log_api_usage_once(adjust_contrast)\n\u001b[32m    906\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[43m.\u001b[49m\u001b[43madjust_contrast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrast_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m F_t.adjust_contrast(img, contrast_factor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/pytorch_study/lib/python3.12/site-packages/torchvision/transforms/_functional_pil.py:83\u001b[39m, in \u001b[36madjust_contrast\u001b[39m\u001b[34m(img, contrast_factor)\u001b[39m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimg should be PIL Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     82\u001b[39m enhancer = ImageEnhance.Contrast(img)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m img = \u001b[43menhancer\u001b[49m\u001b[43m.\u001b[49m\u001b[43menhance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontrast_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/pytorch_study/lib/python3.12/site-packages/PIL/ImageEnhance.py:40\u001b[39m, in \u001b[36m_Enhance.enhance\u001b[39m\u001b[34m(self, factor)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34menhance\u001b[39m(\u001b[38;5;28mself\u001b[39m, factor: \u001b[38;5;28mfloat\u001b[39m) -> Image.Image:\n\u001b[32m     30\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[33;03m    Returns an enhanced image.\u001b[39;00m\n\u001b[32m     32\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     38\u001b[39m \u001b[33;03m    :rtype: :py:class:`~PIL.Image.Image`\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mblend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdegenerate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/anaconda3/envs/pytorch_study/lib/python3.12/site-packages/PIL/Image.py:3614\u001b[39m, in \u001b[36mblend\u001b[39m\u001b[34m(im1, im2, alpha)\u001b[39m\n\u001b[32m   3612\u001b[39m im1.load()\n\u001b[32m   3613\u001b[39m im2.load()\n\u001b[32m-> \u001b[39m\u001b[32m3614\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m im1._new(\u001b[43mcore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mblend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "model = Net()\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epoch_num = 10\n",
    "for epoch in range(epoch_num):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_acc = test(model, test_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{epoch_num}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
